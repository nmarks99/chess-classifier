{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nick Marks\n",
        "# Chess Pieces Classifier\n",
        "\n",
        "A demonstration and walkthrough of the code can be found on YouTube: [https://www.youtube.com/watch?v=GcBdZo7Du3U](https://www.youtube.com/watch?v=GcBdZo7Du3U).\n",
        "\n",
        "Additional code and materials like the dataset used can be found at [https://github.com/nmarks99/chess-classifier](https://github.com/nmarks99/chess-classifier). The `chess_classifier.py` script at this GitHub link is the script shown in the demo video.\n",
        "\n",
        "The goal of this project is to create a program that takes an image of a single chess piece and output the name of that piece. This Jupyter notebook creates and trains the model used for the classification and the `chess_classifier.py` script uses the model to classify the chess piece in a provided image. . Originally, my project proposal suggested that I would make a program that classifies an entire chess board, however this has proven to take longer than expected and I needed to scale the project back slightly. However, this project could be adapted to classify an entire chess board without much trouble. The only additional step that is needed is to create another program that breaks an image of a chess board into 64 images (1 image for each square on the board) and classify each of the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "_cell_guid": "66240b07-f98d-4443-aa2b-855c9ab2f5a9",
        "_uuid": "bb0e65ca-ab55-4d15-a6e0-bc6a040fa868",
        "execution": {
          "iopub.execute_input": "2022-03-14T16:24:57.079316Z",
          "iopub.status.busy": "2022-03-14T16:24:57.078588Z",
          "iopub.status.idle": "2022-03-14T16:24:57.084741Z",
          "shell.execute_reply": "2022-03-14T16:24:57.083768Z",
          "shell.execute_reply.started": "2022-03-14T16:24:57.079255Z"
        },
        "id": "augyEluuEX8E",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "import numpy as np\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "_cell_guid": "66dafa92-4c17-4e67-abda-cfbedcf6b794",
        "_uuid": "fc94861b-96c3-4083-a1cd-b8d84c1e1c0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-03-14T16:24:57.087033Z",
          "iopub.status.busy": "2022-03-14T16:24:57.08675Z",
          "iopub.status.idle": "2022-03-14T16:24:57.106857Z",
          "shell.execute_reply": "2022-03-14T16:24:57.106144Z",
          "shell.execute_reply.started": "2022-03-14T16:24:57.086997Z"
        },
        "id": "tp3BxLB2EX8K",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "0ca059c9-e13f-4c61-d475-516bbf9769d4",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_48160/4162326941.py:6: UserWarning: It is recommended to use train on a GPU, perhaps through Google colab, for performance\n",
            "  warnings.warn(\"It is recommended to use train on a GPU, perhaps through Google colab, for performance\")\n"
          ]
        }
      ],
      "source": [
        "# Setup torch device, using GPU if its available \n",
        "# Training with the CPU on my laptop is very very slow, so using a GPU (perhaps with Google colab) is preferred\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    warnings.warn(\"It is recommended to use train on a GPU, perhaps through Google colab, for performance\")\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"Using {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm2fTDo4yoJi"
      },
      "source": [
        "# Setup\n",
        "To begin, we will start by importing the data. The dataset used in this project was found on Kaggle ([link](https://www.kaggle.com/datasets/anshulmehtakaggl/chess-pieces-detection-images-dataset)) and contains labelled images of chess pieces, including both digital and real images. This is good since I would like my model to have the ability to classify images of chess pieces from online games and well as live over-the-board chess games.\n",
        "\n",
        "The code below imports the data from a chess_pieces directory in the same directory and this project, and separates it into training and validation data. It then creates PyTorch DataLoader objects from the data to be used later on. At this point, this is mostly \"boilerplate\" PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPONYcoqz5r2"
      },
      "source": [
        "## Transforming the Data\n",
        "Some transformations were applied to the data before proceeding which seek to improve the model's ability to classify the images, and make things easier later on. Here I have chosen to apply the RandomHorizontalFlip and RandomRotation transforms. The ToTensor transform is just necessary for using PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l2LSvAUvEX8N"
      },
      "outputs": [],
      "source": [
        "# Define transformations for training\n",
        "input_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomHorizontalFlip(p = 0.4),\n",
        "    transforms.RandomRotation(30),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4aH9UzJ0EX8P"
      },
      "outputs": [],
      "source": [
        "# Load in the dataset\n",
        "dataset_path = \"./datasets/chess_pieces\"\n",
        "dataset = ImageFolder(dataset_path, transform=input_transforms)\n",
        "\n",
        "train_data, val_data = torch.utils.data.random_split(\n",
        "    dataset,\n",
        "    [int(len(dataset)*0.8), len(dataset) - int(len(dataset)*0.8)]\n",
        ")\n",
        "\n",
        "val_data, test_data = torch.utils.data.random_split(\n",
        "    val_data,\n",
        "    [int(len(val_data)*0.8), len(val_data) - int(len(val_data)*0.8)]\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size = 16, shuffle = True)\n",
        "val_loader = DataLoader(val_data, batch_size = 16, shuffle = True)\n",
        "test_loader = DataLoader(test_data, batch_size = 1, shuffle = True)\n",
        "test_loader_ordered = DataLoader(test_data, batch_size = 1, shuffle = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmfjjGtu1VZF"
      },
      "source": [
        "# Defining a Model\n",
        "Throughout our machine learning studies this quarter, we have not discussed deep learning, seeing as that is the topic of next quarter, however, in order to obtain reasonable results for a wide range of chess pieces, I have decided to use a convolutional neural network (CNN) for the model. This is because CNNs are notoriously good at classifying images and although we didn't learn much about them, many of the topics we did learn about still apply. \n",
        "\n",
        "To implement a CNN in PyTorch, we define a Python class that inherits from the nn.Module class. In this case, I have chosen to use the resnet50 model from the torchvision library. Although the mathematics behind this model (a residual neural network, which is a special case of a convolutional neural network) is beyond the scope of my understanding and our EE475 course, I have chosen to use it since implementing it with PyTorch was no more difficult that using another type of model, and residual neural networks have shown to be extremely good at classifying images. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "_cell_guid": "4db1772a-4fc5-48c2-97cc-b00f0e418c20",
        "_uuid": "a6644ddf-144b-4d36-a0e2-8e9a87bd93b4",
        "execution": {
          "iopub.execute_input": "2022-03-14T16:24:57.108527Z",
          "iopub.status.busy": "2022-03-14T16:24:57.108271Z",
          "iopub.status.idle": "2022-03-14T16:24:57.115135Z",
          "shell.execute_reply": "2022-03-14T16:24:57.11424Z",
          "shell.execute_reply.started": "2022-03-14T16:24:57.108493Z"
        },
        "id": "UGBXb8MYEX8Q",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define a neural network as a class that inherits from the torch.nn.Module class \n",
        "class ChessCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ChessCNN, self).__init__()\n",
        "\n",
        "        # use ResNet, a deep neural network model, which is particularly good for image classification\n",
        "        self.model = torchvision.models.resnet50(pretrained = True)\n",
        "\n",
        "        for parameter in self.model.parameters():\n",
        "            parameter.requires_grad = False\n",
        "\n",
        "        # Define the model of each layer TODO: is this correct?\n",
        "        self.model.fc = nn.Sequential(\n",
        "            nn.Linear(2048, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 5)\n",
        "        )\n",
        "\n",
        "    # forward propogation step\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDbqlC2A4l8B"
      },
      "source": [
        "## Defining Training Parameters\n",
        "The model is first instantiated for later use, then we define several parameters like the learning rate, number of iterations, the optimizer that we will use, and the loss function. \n",
        "\n",
        "**Learning Rate**\n",
        "\n",
        "Here we set the learning rate to 0.001. This was chosen after testing several learning rates both higher and lower. Higher learning rates result in faster training, however the loss oscillates or doesn't reach as small a value. An even smaller learning rate may be better, however I have access to only a limited amount of computing power (GPU access through Google Colab) so this learning rate is sufficient for this project.\n",
        "\n",
        "**Number of iterations**\n",
        "The number of iterations was chosen mostly because of time considerations but also and you can see from the output of the training step later on, after several hundred iterations, there are very few new best weights that are found that make the loss function any smaller. \n",
        "\n",
        "**Optimizer**\n",
        "There are many choices for the optimizer to use for this problem. I have chosen two different optimizers to try for this project and to compare performance. The first optimizer I tried was Stochaistic Gradient Descent (SGD), which is much like the standard gradient descent (GD) algorithm we have been using in class, however it will update weights faster than basic gradient descent since it doesn't need to step through the entire training set to update weigths. SGD results in many more oscillations but faster training than GD.\n",
        "\n",
        "The second optimizer I tried was the Adam optimizer. This optimizer is an special implementation of SGD that is based on adaptive estimations of first and second order moments and can be shown to perform better for some problems. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d1600f55-ba5f-431f-b183-db612f9dac68",
        "_uuid": "66e06d27-17d7-4244-9e28-8979c52c791a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-03-14T16:29:59.750758Z",
          "iopub.status.busy": "2022-03-14T16:29:59.750106Z",
          "iopub.status.idle": "2022-03-14T16:30:00.282854Z",
          "shell.execute_reply": "2022-03-14T16:30:00.28215Z",
          "shell.execute_reply.started": "2022-03-14T16:29:59.750683Z"
        },
        "id": "egaO9oxAEX8S",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "1fefed72-a962-4507-cc55-33246f5f42ad",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = ChessCNN() # instantiate the neural net class\n",
        "# learning_rate = 0.00001 # define the learning rate\n",
        "learning_rate = 0.001\n",
        "max_its = 1000 \n",
        "\n",
        "# Define the optimizer\n",
        "# optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 0.001)\n",
        "optimizer = optim.SGD(model.parameters(),lr=learning_rate,momentum=0.01)\n",
        "\n",
        "# Define the loss function\n",
        "loss_func = nn.CrossEntropyLoss() # use cross entropy loss function\n",
        "min_loss = np.inf\n",
        "model.to(device) # set model to use the appropriate defice (GPU or CPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training\n",
        "To train the model using the chosen loss function and optimizer (SGD or Adam) I created the function below to make it easier. The training function loops through the data in the training set, computes the loss, and updates the weights. It then uses the validation set to check the performance at each step and if its better than the previous best performing model, it will save the current model as the new best. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "_cell_guid": "7e11bfe7-4a3a-449f-b9a5-6daa591956f1",
        "_uuid": "d2af0b73-a6f0-48a9-a844-d3b454eb6985",
        "execution": {
          "iopub.execute_input": "2022-03-14T16:29:38.98681Z",
          "iopub.status.busy": "2022-03-14T16:29:38.986272Z",
          "iopub.status.idle": "2022-03-14T16:29:38.99915Z",
          "shell.execute_reply": "2022-03-14T16:29:38.998375Z",
          "shell.execute_reply.started": "2022-03-14T16:29:38.986771Z"
        },
        "id": "8pWcNsNGEX8U",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train(model, max_its, min_loss):\n",
        "    for step in range(max_its):\n",
        "        training_loss = 0\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            yp = model(images)\n",
        "            loss = loss_func(yp, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            training_loss += loss.item()\n",
        "            \n",
        "            del images, labels\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "\n",
        "        valid_loss = 0\n",
        "        valid_accuracy = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                yp = model(images)\n",
        "                loss = loss_func(yp, labels)\n",
        "                valid_loss += loss.item()\n",
        "                yp = nn.Softmax(dim = 1)(yp)\n",
        "                _, top_class = yp.topk(1, dim = 1)\n",
        "                num_correct = top_class == labels.view(-1,1)\n",
        "                valid_accuracy += num_correct.sum().item()\n",
        "\n",
        "                # clear data to ensure there isn't confusion\n",
        "                del(images)\n",
        "                del(labels)\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # print(\"Step: {} \\tTraining loss: {:.4f} \\tValidation loss: {:.4f} \\tAccuracy: {:.2f}%\".format(step, training_loss, valid_loss, (valid_accuracy/len(val_data))*100))\n",
        "        print(f\"Step: {step} \\tTraining loss: {training_loss:.4f} \\tValidation loss: {valid_loss:.4f} \\tAccuracy: {valid_accuracy/len(val_data)*100:.2f}%\")\n",
        "\n",
        "        # whenever a new minimum loss for the model is found replace the previous best model\n",
        "        if valid_loss <= min_loss:\n",
        "            print(f\"New minumum loss found! = {valid_loss:.4f}\\tSaving model...\")\n",
        "            torch.save(model.state_dict(), \"trained_model.pt\")\n",
        "            min_loss = valid_loss # set new minumum loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrIoWzMi-nBr"
      },
      "source": [
        "# Training\n",
        "Running the cell below will begin the training process. This can be very slow, especially on an average CPU. Therefore, to train the model I have chosen to use Google Colab which offers free (but limited) GPU usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "390812cb-c782-49ea-a31f-9e02f66b696f",
        "_uuid": "0a81eef0-af1a-4588-bd0c-5adadc3e6d02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2022-03-14T16:29:39.587801Z",
          "iopub.status.busy": "2022-03-14T16:29:39.587267Z",
          "iopub.status.idle": "2022-03-14T16:29:48.921548Z",
          "shell.execute_reply": "2022-03-14T16:29:48.92036Z",
          "shell.execute_reply.started": "2022-03-14T16:29:39.587756Z"
        },
        "id": "QXbpKApuEX8W",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "33daa680-d1c5-4be3-e6d1-a91bc2c726a7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train(model, max_its, min_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results\n",
        "In the cells below we evaluate the models, (one with the Adam optimizer and one with the SGD optimizer) with the testing data set. As you can see from the printout of the results, both models perform very similarly, so for this project, the SGD and Adam optimizer are comparable. \n",
        "\n",
        "The final accuracy of the trained models tops out at just under 80%. This is decent performance considering the dataset and the training time. Better performance most likely could be acheived with a larger more comprehensive dataset and longer training time to allow the loss to decrease even further. The training time and computational requirements were definitely a limitation on this project, since to achieve these results, training took at least 1 hour running on Google Colab's provided GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "_cell_guid": "de154d7e-1447-455b-b4a7-28bd514bf366",
        "_uuid": "d11e3af8-2c8f-46d5-8b0f-269c38121410",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-03-14T16:31:02.333657Z",
          "iopub.status.busy": "2022-03-14T16:31:02.333404Z",
          "iopub.status.idle": "2022-03-14T16:31:03.894833Z",
          "shell.execute_reply": "2022-03-14T16:31:03.894065Z",
          "shell.execute_reply.started": "2022-03-14T16:31:02.333627Z"
        },
        "id": "18b2mMlcEX8X",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "60a3aac5-98de-4946-b57f-7bb500d7f3f8",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===========================\n",
            "Results with SGD optimizer:\n",
            "===========================\n",
            "Prediction: Rook-resize\t Confidence: 43.09%\t Actual label: pawn_resized\n",
            "Prediction: knight-resize\t Confidence: 99.70%\t Actual label: knight-resize\n",
            "Prediction: Rook-resize\t Confidence: 86.18%\t Actual label: Rook-resize\n",
            "Prediction: Queen-Resized\t Confidence: 80.27%\t Actual label: Queen-Resized\n",
            "Prediction: bishop_resized\t Confidence: 92.43%\t Actual label: bishop_resized\n",
            "Prediction: knight-resize\t Confidence: 72.78%\t Actual label: knight-resize\n",
            "Prediction: knight-resize\t Confidence: 76.24%\t Actual label: knight-resize\n",
            "Prediction: knight-resize\t Confidence: 85.55%\t Actual label: knight-resize\n",
            "Prediction: Queen-Resized\t Confidence: 89.55%\t Actual label: Queen-Resized\n",
            "Prediction: Queen-Resized\t Confidence: 58.92%\t Actual label: bishop_resized\n",
            "Prediction: Queen-Resized\t Confidence: 68.47%\t Actual label: Queen-Resized\n",
            "Prediction: bishop_resized\t Confidence: 76.25%\t Actual label: bishop_resized\n",
            "Prediction: knight-resize\t Confidence: 77.08%\t Actual label: knight-resize\n",
            "Prediction: Rook-resize\t Confidence: 99.44%\t Actual label: Rook-resize\n",
            "Prediction: knight-resize\t Confidence: 86.49%\t Actual label: knight-resize\n",
            "Prediction: knight-resize\t Confidence: 91.18%\t Actual label: knight-resize\n",
            "Prediction: Queen-Resized\t Confidence: 74.00%\t Actual label: bishop_resized\n",
            "Prediction: bishop_resized\t Confidence: 79.27%\t Actual label: Rook-resize\n",
            "Prediction: Rook-resize\t Confidence: 66.44%\t Actual label: Rook-resize\n",
            "Prediction: Rook-resize\t Confidence: 57.77%\t Actual label: bishop_resized\n",
            "Prediction: pawn_resized\t Confidence: 52.18%\t Actual label: pawn_resized\n",
            "Prediction: Queen-Resized\t Confidence: 95.31%\t Actual label: Queen-Resized\n",
            "Prediction: knight-resize\t Confidence: 81.42%\t Actual label: knight-resize\n",
            "Prediction: pawn_resized\t Confidence: 84.91%\t Actual label: pawn_resized\n",
            "Prediction: Queen-Resized\t Confidence: 58.78%\t Actual label: bishop_resized\n",
            "Prediction: knight-resize\t Confidence: 85.80%\t Actual label: knight-resize\n",
            "Prediction: Queen-Resized\t Confidence: 56.27%\t Actual label: Queen-Resized\n",
            "Accuracy:\n",
            "=========\n",
            "Score 21/27\n",
            "Accuracy: 77.78%\n"
          ]
        }
      ],
      "source": [
        "total_correct = 0\n",
        "count = 0\n",
        "classes = dataset.classes\n",
        "model.load_state_dict(torch.load('./model_SGD.pt',map_location=device))\n",
        "\n",
        "print(\"===========================\")\n",
        "print(\"Results with SGD optimizer:\")\n",
        "print(\"===========================\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        yp = model(images)\n",
        "        yp = nn.Softmax(dim = 1)(yp)\n",
        "        top_p, top_class = yp.topk(1, dim = 1)\n",
        "        eq = top_class == labels.view(-1, 1)\n",
        "        # print(classes[top_class.item()])\n",
        "        total_correct += eq.sum().item()\n",
        "        \n",
        "        if count % 1 == 0:\n",
        "            print(\"Prediction: {}\\t Confidence: {:.2f}%\\t Actual label: {}\".format(classes[top_class.item()], top_p.item() * 100, classes[labels.item()]))\n",
        "        else:\n",
        "            print(f\"count%1 = {count % 1}\")\n",
        "        count += 1\n",
        "\n",
        "print(\"Accuracy:\\n=========\")\n",
        "print(f\"Score {total_correct}/{len(test_data)}\")\n",
        "print(f\"Accuracy: {(total_correct/len(test_data)) * 100:.2f}%\")\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===========================\n",
            "Results with Adam optimizer:\n",
            "===========================\n",
            "Prediction: knight-resize\t Confidence: 98.64%\t Actual label: knight-resize\n",
            "Prediction: knight-resize\t Confidence: 99.38%\t Actual label: knight-resize\n",
            "Prediction: pawn_resized\t Confidence: 41.99%\t Actual label: pawn_resized\n",
            "Prediction: knight-resize\t Confidence: 98.99%\t Actual label: knight-resize\n",
            "Prediction: Queen-Resized\t Confidence: 81.18%\t Actual label: Queen-Resized\n",
            "Prediction: bishop_resized\t Confidence: 47.86%\t Actual label: Rook-resize\n",
            "Prediction: bishop_resized\t Confidence: 76.42%\t Actual label: bishop_resized\n",
            "Prediction: Rook-resize\t Confidence: 75.67%\t Actual label: Rook-resize\n",
            "Prediction: knight-resize\t Confidence: 99.30%\t Actual label: knight-resize\n",
            "Prediction: knight-resize\t Confidence: 95.13%\t Actual label: knight-resize\n",
            "Prediction: Queen-Resized\t Confidence: 96.16%\t Actual label: Queen-Resized\n",
            "Prediction: bishop_resized\t Confidence: 37.09%\t Actual label: bishop_resized\n",
            "Prediction: bishop_resized\t Confidence: 44.53%\t Actual label: Queen-Resized\n",
            "Prediction: pawn_resized\t Confidence: 97.79%\t Actual label: pawn_resized\n",
            "Prediction: knight-resize\t Confidence: 72.79%\t Actual label: knight-resize\n",
            "Prediction: Rook-resize\t Confidence: 86.53%\t Actual label: Rook-resize\n",
            "Prediction: knight-resize\t Confidence: 98.90%\t Actual label: knight-resize\n",
            "Prediction: Queen-Resized\t Confidence: 41.97%\t Actual label: bishop_resized\n",
            "Prediction: Queen-Resized\t Confidence: 59.26%\t Actual label: Queen-Resized\n",
            "Prediction: bishop_resized\t Confidence: 59.03%\t Actual label: bishop_resized\n",
            "Prediction: Queen-Resized\t Confidence: 51.27%\t Actual label: Queen-Resized\n",
            "Prediction: Rook-resize\t Confidence: 37.03%\t Actual label: knight-resize\n",
            "Prediction: Rook-resize\t Confidence: 98.09%\t Actual label: Rook-resize\n",
            "Prediction: Rook-resize\t Confidence: 47.88%\t Actual label: pawn_resized\n",
            "Prediction: pawn_resized\t Confidence: 61.92%\t Actual label: knight-resize\n",
            "Prediction: Rook-resize\t Confidence: 62.42%\t Actual label: bishop_resized\n",
            "Prediction: bishop_resized\t Confidence: 90.03%\t Actual label: bishop_resized\n",
            "Accuracy:\n",
            "=========\n",
            "Score: 20/27\n",
            "Accuracy: 74.07%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "total_correct = 0\n",
        "count = 0\n",
        "classes = dataset.classes\n",
        "model.load_state_dict(torch.load('./model_adam.pt',map_location=device))\n",
        "\n",
        "print(\"===========================\")\n",
        "print(\"Results with Adam optimizer:\")\n",
        "print(\"===========================\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        yp = model(images)\n",
        "        yp = nn.Softmax(dim = 1)(yp)\n",
        "        top_p, top_class = yp.topk(1, dim = 1)\n",
        "        eq = top_class == labels.view(-1, 1)\n",
        "        # print(classes[top_class.item()])\n",
        "        total_correct += eq.sum().item()\n",
        "        \n",
        "        if count % 1 == 0:\n",
        "            print(\"Prediction: {}\\t Confidence: {:.2f}%\\t Actual label: {}\".format(classes[top_class.item()], top_p.item() * 100, classes[labels.item()]))\n",
        "        else:\n",
        "            print(f\"count%1 = {count % 1}\")\n",
        "        count += 1\n",
        "\n",
        "print(\"Accuracy:\\n=========\")\n",
        "print(f\"Score: {total_correct}/{len(test_data)}\")\n",
        "print(f\"Accuracy: {(total_correct/len(test_data)) * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
